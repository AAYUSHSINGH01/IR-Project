{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json, nltk, keras\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vqa annotations\n",
    "vqa_annot = {'train': {}, 'val': {}, 'test': {}}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    with open('vqa_annotations/{}.json'.format(split), encoding=\"utf8\") as file:\n",
    "        annot = json.load(file)\n",
    "        for entry in annot:\n",
    "            temp = {'question': entry['question']} \n",
    "            if split != 'test':\n",
    "                temp['answerable'] = float(entry['answerable'])\n",
    "            vqa_annot[split][entry['image']] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 2\n",
    "# load quality annotations\n",
    "quality_annot = {'train': {}, 'val': {}, 'test': {}}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    with open('quality_annotations/{}.json'.format(split)) as file:\n",
    "        annot = json.load(file)\n",
    "        for entry in annot:\n",
    "            temp = {} \n",
    "            if split != 'test':\n",
    "                flaws = entry['flaws']\n",
    "                # check flaws.keys() \n",
    "                temp['flaws'] = np.array(list(flaws.values())) >= THRESHOLD\n",
    "                temp['recognizable'] = float(1 - (entry['unrecognizable'] >= THRESHOLD))\n",
    "            quality_annot[split][entry['image']] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge vqa and quality annotations\n",
    "# note that vqa dataset is smaller than quality dataset and\n",
    "# 1. vqa training set is NOT exactly a subset of quality training set\n",
    "# 2. vqa validation set is a subset of quality validation set\n",
    "# 3. vqa testing set is the same as quality testing set\n",
    "merged_annot = {'train': {}, 'val': {}, 'test': {}}\n",
    "for split in ['train', 'val']:\n",
    "    vqa_split, quality_split = vqa_annot[split], quality_annot[split]\n",
    "    for fname in vqa_split:\n",
    "        if quality_split.get(fname):\n",
    "            merged_annot[split][fname] = {**vqa_split[fname], **quality_split[fname]}\n",
    "merged_annot['test'] = vqa_annot['test'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert quality_annot/merged_annot from dictionary to array for further use\n",
    "# First, we need a function to numerically encode questions\n",
    "\n",
    "vocab = json.load(open('./utils/word2vocab_vizwiz.json'))\n",
    "def encode_sentence(sentence, vocab=vocab, max_len = 14):\n",
    "    unk_word = '<UNK>'\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    tokens_id = [vocab.get(x, vocab[unk_word]) + 1 for x in tokens] # +1 to reserve 0 for zero paddings\n",
    "    padded_tokens_id = keras.preprocessing.sequence.pad_sequences(\n",
    "                        [tokens_id], maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "    return padded_tokens_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_annot_array = {}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    annot = {'image': [], 'flaws': [], 'recognizable': []}\n",
    "    _split = quality_annot[split]\n",
    "    for fname in _split:\n",
    "        annot['image'].append(fname)        \n",
    "        if split != 'test':            \n",
    "            annot['flaws'].append(_split[fname].get('flaws').tolist()) \n",
    "            annot['recognizable'].append([_split[fname].get('recognizable')])\n",
    "    quality_annot_array[split] = annot\n",
    "    \n",
    "with open('data/quality.json', 'w') as outfile:\n",
    "    json.dump(quality_annot_array, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_annot_array = {}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    annot = {'image': [], 'answerable': [], 'flaws': [], 'question': [], 'recognizable': []}\n",
    "    _split = merged_annot[split]\n",
    "    for fname in _split:\n",
    "        annot['image'].append(fname)\n",
    "        annot['question'].append(encode_sentence(_split[fname].get('question')).tolist())\n",
    "        if split != 'test':\n",
    "            annot['answerable'].append([_split[fname].get('answerable')])\n",
    "            annot['flaws'].append(_split[fname].get('flaws').tolist()) \n",
    "            annot['recognizable'].append([_split[fname].get('recognizable')])\n",
    "    merged_annot_array[split] = annot\n",
    "    \n",
    "with open('data/vqa_quality_merger.json', 'w') as outfile:\n",
    "    json.dump(merged_annot_array, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
