{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json, nltk, keras\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vqa_annotations/train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m vqa_annot \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: {}, \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m: {}, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m: {}}\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mvqa_annotations/\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m.json\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(split), encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m      5\u001b[0m         annot \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(file)\n\u001b[0;32m      6\u001b[0m         \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m annot:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vqa_annotations/train.json'"
     ]
    }
   ],
   "source": [
    "# load vqa annotations\n",
    "vqa_annot = {'train': {}, 'val': {}, 'test': {}}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    with open('vqa_annotations/{}.json'.format(split), encoding=\"utf8\") as file:\n",
    "        annot = json.load(file)\n",
    "        for entry in annot:\n",
    "            temp = {'question': entry['question']} \n",
    "            if split != 'test':\n",
    "                temp['answerable'] = float(entry['answerable'])\n",
    "            vqa_annot[split][entry['image']] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 2\n",
    "# load quality annotations\n",
    "quality_annot = {'train': {}, 'val': {}, 'test': {}}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    with open('quality_annotations/{}.json'.format(split)) as file:\n",
    "        annot = json.load(file)\n",
    "        for entry in annot:\n",
    "            temp = {} \n",
    "            if split != 'test':\n",
    "                flaws = entry['flaws']\n",
    "                # check flaws.keys() \n",
    "                temp['flaws'] = np.array(list(flaws.values())) >= THRESHOLD\n",
    "                temp['recognizable'] = float(1 - (entry['unrecognizable'] >= THRESHOLD))\n",
    "            quality_annot[split][entry['image']] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge vqa and quality annotations\n",
    "# note that vqa dataset is smaller than quality dataset and\n",
    "# 1. vqa training set is NOT exactly a subset of quality training set\n",
    "# 2. vqa validation set is a subset of quality validation set\n",
    "# 3. vqa testing set is the same as quality testing set\n",
    "merged_annot = {'train': {}, 'val': {}, 'test': {}}\n",
    "for split in ['train', 'val']:\n",
    "    vqa_split, quality_split = vqa_annot[split], quality_annot[split]\n",
    "    for fname in vqa_split:\n",
    "        if quality_split.get(fname):\n",
    "            merged_annot[split][fname] = {**vqa_split[fname], **quality_split[fname]}\n",
    "merged_annot['test'] = vqa_annot['test'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert quality_annot/merged_annot from dictionary to array for further use\n",
    "# First, we need a function to numerically encode questions\n",
    "\n",
    "vocab = json.load(open('./utils/word2vocab_vizwiz.json'))\n",
    "def encode_sentence(sentence, vocab=vocab, max_len = 14):\n",
    "    unk_word = '<UNK>'\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    tokens_id = [vocab.get(x, vocab[unk_word]) + 1 for x in tokens] # +1 to reserve 0 for zero paddings\n",
    "    padded_tokens_id = keras.preprocessing.sequence.pad_sequences(\n",
    "                        [tokens_id], maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "    return padded_tokens_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_annot_array = {}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    annot = {'image': [], 'flaws': [], 'recognizable': []}\n",
    "    _split = quality_annot[split]\n",
    "    for fname in _split:\n",
    "        annot['image'].append(fname)        \n",
    "        if split != 'test':            \n",
    "            annot['flaws'].append(_split[fname].get('flaws').tolist()) \n",
    "            annot['recognizable'].append([_split[fname].get('recognizable')])\n",
    "    quality_annot_array[split] = annot\n",
    "    \n",
    "with open('data/quality.json', 'w') as outfile:\n",
    "    json.dump(quality_annot_array, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_annot_array = {}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    annot = {'image': [], 'answerable': [], 'flaws': [], 'question': [], 'recognizable': []}\n",
    "    _split = merged_annot[split]\n",
    "    for fname in _split:\n",
    "        annot['image'].append(fname)\n",
    "        annot['question'].append(encode_sentence(_split[fname].get('question')).tolist())\n",
    "        if split != 'test':\n",
    "            annot['answerable'].append([_split[fname].get('answerable')])\n",
    "            annot['flaws'].append(_split[fname].get('flaws').tolist()) \n",
    "            annot['recognizable'].append([_split[fname].get('recognizable')])\n",
    "    merged_annot_array[split] = annot\n",
    "    \n",
    "with open('data/vqa_quality_merger.json', 'w') as outfile:\n",
    "    json.dump(merged_annot_array, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
